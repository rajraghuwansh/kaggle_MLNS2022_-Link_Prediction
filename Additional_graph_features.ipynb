{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef4dc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rkroc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rkroc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from utils import common, overlap, loop, parallel_loop, overlap_df\n",
    "import igraph as ig\n",
    "import networkx as nx\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b598b",
   "metadata": {},
   "source": [
    "#### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jacc_Similarity(row, field):\n",
    "    text1 = row[field+'_target'].lower()\n",
    "    text2 = row[field+'_source'].lower()\n",
    "    intersection = set(text1).intersection(set(text2))\n",
    "    union = set(text1).union(set(text2))\n",
    "    return float(len(intersection)) / len(union)\n",
    "\n",
    "def overlap(row, field):\n",
    "    text1 = row[field+'_target']\n",
    "    text2 = row[field+'_source']\n",
    "    text1 = stop_words_stems(text1)\n",
    "    text2 = stop_words_stems(text2)\n",
    "    overlap = len(set(text1).intersection(set(text2)))\n",
    "    return overlap\n",
    "\n",
    "def overlap_df(df, name='Overlap_title', field='Title'):\n",
    "    df[name] = df.apply(lambda row : overlap(row, field))\n",
    "    return df\n",
    "\n",
    "\n",
    "def loop(df, f, field):\n",
    "    l=[]\n",
    "    le = len(df)\n",
    "    for index, row in df.iterrows():\n",
    "        l.append(f(row, field))\n",
    "        if index%10000==0:\n",
    "            print(index, le)\n",
    "    return f\n",
    "\n",
    "def parallel_loop(df, f):\n",
    "    partitions = 2\n",
    "    data_split = np.array_split(df, partitions)\n",
    "    pool = Pool(2)\n",
    "    df = pd.concat(pool.map(f, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "    \n",
    "\n",
    "def common(row, field='Authors'):\n",
    "    text1 = row[field+'_target']\n",
    "    text2 = row[field+'_source']\n",
    "    if text1!=text1 or text2!=text2:\n",
    "        return 0\n",
    "    text1 = text1.split(\",\")\n",
    "    text2 = text2.split(\",\")\n",
    "    common = len(set(text1).intersection(set(text2)))\n",
    "    return common\n",
    "    \n",
    "def stop_words_stems(txt):\n",
    "    txt = txt.split(\",\")\n",
    "    txt = [token for token in txt if token not in stpwds]\n",
    "    txt = [stemmer.stem(token) for token in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29bc0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_information = pd.read_csv('node_information.csv', header=None, names=['ID', 'Year', 'Title', 'Authors', 'Journal', 'Abstract'])\n",
    "training_set = pd.read_csv('training_set.txt', header=None, names=['Target', 'Source', 'Edge'], delim_whitespace=True)\n",
    "testing_set = pd.read_csv('testing_set.txt', header=None, names=['Target', 'Source', 'Edge'], delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581c6163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID          0\n",
       "Year        0\n",
       "Title       0\n",
       "Authors     0\n",
       "Journal     0\n",
       "Abstract    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_information.fillna('',inplace=True)\n",
    "node_information.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a185e610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Source</th>\n",
       "      <th>Edge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9807076</td>\n",
       "      <td>9807139</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109162</td>\n",
       "      <td>1182</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9702187</td>\n",
       "      <td>9510135</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111048</td>\n",
       "      <td>110115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9910176</td>\n",
       "      <td>9410073</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32643</th>\n",
       "      <td>9705209</td>\n",
       "      <td>9305083</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32644</th>\n",
       "      <td>9307023</td>\n",
       "      <td>9503118</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32645</th>\n",
       "      <td>9608095</td>\n",
       "      <td>9205058</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32646</th>\n",
       "      <td>9407008</td>\n",
       "      <td>106256</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32647</th>\n",
       "      <td>208144</td>\n",
       "      <td>7142</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32648 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Target   Source  Edge\n",
       "0      9807076  9807139   NaN\n",
       "1       109162     1182   NaN\n",
       "2      9702187  9510135   NaN\n",
       "3       111048   110115   NaN\n",
       "4      9910176  9410073   NaN\n",
       "...        ...      ...   ...\n",
       "32643  9705209  9305083   NaN\n",
       "32644  9307023  9503118   NaN\n",
       "32645  9608095  9205058   NaN\n",
       "32646  9407008   106256   NaN\n",
       "32647   208144     7142   NaN\n",
       "\n",
       "[32648 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6a7c7d",
   "metadata": {},
   "source": [
    "##### basic text features already created can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74367450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    print(\"Get valid IDs\")\n",
    "    valid_ids=set()\n",
    "    for element in training_set.values:\n",
    "        valid_ids.add(element[0])\n",
    "        valid_ids.add(element[1])\n",
    "        \n",
    "    print(\"Select valid indices from valid IDs\")\n",
    "    index_valid=[i for i, element in enumerate(node_information.values) if element[0] in valid_ids]\n",
    "    node_info=node_information.iloc[index_valid]\n",
    "    \n",
    "    print(\"Get index for nodes\")\n",
    "    IDs = []\n",
    "    ID_pos={}\n",
    "    for element in node_info.values:\n",
    "        ID_pos[element[0]]=len(IDs)\n",
    "        IDs.append(element[0])\n",
    "        \n",
    "    print(\"Add ID column for merging\")\n",
    "    training_set['Target_ID']= training_set.apply(lambda row : ID_pos[row[0]], axis=1)#\n",
    "    training_set['Source_ID']= training_set.apply(lambda row : ID_pos[row[1]], axis=1)#\n",
    "    \n",
    "    print(\"Merge\")\n",
    "    train = pd.merge(training_set, node_information, how='left', left_on='Target_ID', right_index=True)\n",
    "    train = pd.merge(train, node_information, how='left', left_on='Source_ID', right_index=True, suffixes=['_target', '_source'])\n",
    "    train.to_csv('train_basic_tfidfs.csv', index=False)\n",
    "   \n",
    "    \n",
    "    train['Overlap_title'] = train.apply(lambda row :overlap(row, 'Title'), axis=1)#\n",
    " \n",
    "    train.to_csv('train_basic_tfidfs.csv', index=False)\n",
    "    train['Overlap_abstract'] = train.apply(lambda row :overlap(row, 'Abstract'), axis=1)#\n",
    "    \n",
    "    train.to_csv('train_basic_tfidfs.csv', index=False)\n",
    "    train['Overlap_journal'] = train.apply(lambda row :overlap(row, 'Journal'), axis=1)#\n",
    "    \n",
    "    train.to_csv('train_basic_tfidfs.csv', index=False)\n",
    "    train['Common_authors'] = train.apply(lambda row :common(row, 'Authors'), axis=1)\n",
    "\n",
    "    train.to_csv('train_basic_tfidfs.csv', index=False)\n",
    "\n",
    "    train['Common_authors_prob'] = train.apply(lambda row :Jacc_Similarity(row, 'Authors'), axis=1)\n",
    "    print(time()-t)\n",
    "    train.to_csv('train_basic_tfidfs.csv', index=False)\n",
    "    train['Common_journal_prob'] = train.apply(lambda row :Jacc_Similarity(row, 'Journal'), axis=1)#\n",
    "    train.to_csv('train_basic_tfidfs.csv', index=False)\n",
    "    train['Date_diff'] = (train['Year_source']-train['Year_target']).abs()#\n",
    "    train.to_csv('train_basic_tfidfs.csv', index=False)\n",
    "   \n",
    "    return train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9661f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_semantics = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0139cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    \n",
    "    print(\"Get valid IDs\")\n",
    "    valid_ids=set()\n",
    "    for element in training_set.values:\n",
    "        valid_ids.add(element[0])\n",
    "        valid_ids.add(element[1])\n",
    "        \n",
    "    print(\"Select valid indices from valid IDs\")\n",
    "    index_valid=[i for i, element in enumerate(node_information.values) if element[0] in valid_ids]\n",
    "    node_info=node_information.iloc[index_valid]\n",
    "    \n",
    "    print(\"Get index for nodes\")\n",
    "    IDs = []\n",
    "    ID_pos={}\n",
    "    for element in node_info.values:\n",
    "        ID_pos[element[0]]=len(IDs)\n",
    "        IDs.append(element[0])\n",
    "        \n",
    "    print(\"Add ID column for merging\")\n",
    "    testing_set['Target_ID']= testing_set.apply(lambda row : ID_pos[row[0]], axis=1)#\n",
    "    testing_set['Source_ID']= testing_set.apply(lambda row : ID_pos[row[1]], axis=1)#\n",
    "    \n",
    "    print(\"Merge\")\n",
    "    test = pd.merge(testing_set, node_information, how='left', left_on='Target_ID', right_index=True)\n",
    "    test = pd.merge(test, node_information, how='left', left_on='Source_ID', right_index=True, suffixes=['_target', '_source'])\n",
    "    test.to_csv('test_basic_tfidfs.csv', index=False)\n",
    "    \n",
    "    t = time()\n",
    "    \n",
    "    test['Overlap_title'] = test.apply(lambda row :overlap(row, 'Title'), axis=1)#\n",
    "    test['Overlap_abstract'] = test.apply(lambda row :overlap(row, 'Abstract'), axis=1)#\n",
    "    test['Overlap_journal'] = test.apply(lambda row :overlap(row, 'Journal'), axis=1)#\n",
    "    \n",
    "    test['Common_authors'] = test.apply(lambda row :common(row, 'Authors'), axis=1)\n",
    "    test['Common_authors_prob'] = test.apply(lambda row :Jacc_Similarity(row, 'Authors'), axis=1)\n",
    "    test['Common_journal_prob'] = test.apply(lambda row :Jacc_Similarity(row, 'Journal'), axis=1)#\n",
    "    \n",
    "    test['Date_diff'] = (test['Year_source']-test['Year_target']).abs()#\n",
    "    print(time()-t)\n",
    "    test.to_csv('test_basic_tfidfs.csv', index=False)\n",
    "    \n",
    "    \n",
    "\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5405f6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get valid IDs\n",
      "Select valid indices from valid IDs\n",
      "Get index for nodes\n",
      "Add ID column for merging\n",
      "Merge\n",
      "14.434291362762451\n"
     ]
    }
   ],
   "source": [
    "test_semantics = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63890097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Target', 'Source', 'Edge', 'Target_ID', 'Source_ID', 'ID_target',\n",
       "       'Year_target', 'Title_target', 'Authors_target', 'Journal_target',\n",
       "       'Abstract_target', 'ID_source', 'Year_source', 'Title_source',\n",
       "       'Authors_source', 'Journal_source', 'Abstract_source', 'Overlap_title',\n",
       "       'Overlap_abstract', 'Overlap_journal', 'Common_authors',\n",
       "       'Common_authors_prob', 'Common_journal_prob', 'Date_diff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_semantics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e1d1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_semantics = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20af0f6",
   "metadata": {},
   "source": [
    "# Additional_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "68d4ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(X, y):\n",
    "    graph = nx.Graph()\n",
    "    edges=[]\n",
    "    nodes=set()\n",
    "    for i in range(len(X)):\n",
    "        source = X[i][0]\n",
    "        target = X[i][1]\n",
    "        nodes.add(source)\n",
    "        nodes.add(target)\n",
    "        if y[i]==1:\n",
    "            edges.append((source, target))\n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_edges_from(edges)\n",
    "    return graph\n",
    "\n",
    "def create_directed_graph(X, y):\n",
    "    graph = nx.DiGraph()\n",
    "    edges=[]\n",
    "    nodes=set()\n",
    "    for i in range(len(X)):\n",
    "        source = X[i][0]\n",
    "        target = X[i][1]\n",
    "        nodes.add(source)\n",
    "        nodes.add(target)\n",
    "        if y[i]==1:\n",
    "            edges.append((source, target))\n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_edges_from(edges)\n",
    "    return graph\n",
    "\n",
    "def vertex_degree(graph, v):\n",
    "    return graph.degree(v)\n",
    "\n",
    "def count(graph, nodes):\n",
    "    c=0\n",
    "    for node in nodes:\n",
    "        for node_ in nodes:\n",
    "            if graph.has_edge(node, node_):\n",
    "                c+=1\n",
    "    return c\n",
    "def subgraphs_edge_number(graph, v):\n",
    "    #neighbors = graph.neighbors(v)\n",
    "    neighbors = list(graph[v].keys())\n",
    "    neighbors_plus = neighbors + [v]\n",
    "    subgraph = graph.subgraph(neighbors)\n",
    "    subgraph_plus = graph.subgraph(neighbors_plus)\n",
    "    sub_edge_num = subgraph.number_of_edges()\n",
    "    #sub_edge_num = subgraph.size()\n",
    "    sub_edge_num_plus = subgraph_plus.number_of_edges()\n",
    "    return sub_edge_num, sub_edge_num_plus\n",
    "\n",
    "def all_vertex(graph, v):\n",
    "    neighbors = list(graph[v].keys())\n",
    "    neighbors_plus = neighbors + [v]\n",
    "    subgraph = graph.subgraph(neighbors)\n",
    "    subgraph_plus = graph.subgraph(neighbors_plus)\n",
    "    sub_edge_num = subgraph.number_of_edges()\n",
    "    sub_edge_num_plus = subgraph_plus.number_of_edges()\n",
    "    #sub_edge_num = count(graph, neighbors)\n",
    "    #sub_edge_num_plus = count(graph, neighbors_plus)\n",
    "    return graph.degree(v), sub_edge_num, sub_edge_num_plus\n",
    "\n",
    "def common_friends(graph, u, v):\n",
    "    return len(nx.common_neighbors(graph, u, v))\n",
    "\n",
    "def total_friends(graph, u, v):\n",
    "    neighbors_u = list(graph[u].keys())\n",
    "    neighbors_v = list(graph[v].keys())\n",
    "    total = list(set(neighbors_u).union(neighbors_v))\n",
    "    return len(total)\n",
    "\n",
    "def friends_measure(graph, u, v):\n",
    "    neighbors_u = list(graph[u].keys())\n",
    "    neighbors_v = list(graph[v].keys())\n",
    "    c=0\n",
    "    for n_u in neighbors_u:\n",
    "        for n_v in neighbors_v:\n",
    "            if graph.has_edge(n_u, n_v) or graph.has_edge(n_v, n_u):\n",
    "                c+=1\n",
    "                \n",
    "def subgraph_features(graph, u, v):\n",
    "    neighbors_u = list(graph[u].keys())\n",
    "    neighbors_v = list(graph[v].keys())\n",
    "    neighbors_u_plus = neighbors_u + [u]\n",
    "    neighbors_v_plus = neighbors_v + [v]\n",
    "    nh = list(set(neighbors_u).union(neighbors_v))\n",
    "    nh_plus = list(set(neighbors_u_plus).union(neighbors_v_plus))\n",
    "    sub_nh = graph.subgraph(nh)\n",
    "    sub_nh_plus = graph.subgraph(nh_plus)\n",
    "    return sub_nh.number_of_edges(), sub_nh_plus.number_of_edges()\n",
    "\n",
    "def shortest_path(graph, u, v):\n",
    "    return nx.shortest_path_length(graph, u, v)\n",
    "\n",
    "def all_edges(graph, u, v):\n",
    "    common_friends = len(list(nx.common_neighbors(graph, u, v)))\n",
    "    neighbors_u = list(graph[u].keys())\n",
    "    neighbors_v = list(graph[v].keys())\n",
    "    nh = list(set(neighbors_u).union(neighbors_v))\n",
    "    total_friends = len(nh)\n",
    "    friends_measure=0\n",
    "    for n_u in neighbors_u:\n",
    "        for n_v in neighbors_v:\n",
    "            if graph.has_edge(n_u, n_v) or graph.has_edge(n_v, n_u):\n",
    "                friends_measure+=1\n",
    "    neighbors_u_plus = neighbors_u + [u]\n",
    "    neighbors_v_plus = neighbors_v + [v]\n",
    "    nh_plus = list(set(neighbors_u_plus).union(neighbors_v_plus))\n",
    "    sub_nh = graph.subgraph(nh)\n",
    "    sub_nh_plus = graph.subgraph(nh_plus)\n",
    "    if not nx.has_path(graph, v, u):\n",
    "        len_path=-1\n",
    "    else:\n",
    "        len_path = nx.shortest_path_length(graph, v, u)\n",
    "    return common_friends, total_friends, friends_measure, sub_nh.number_of_edges(), sub_nh_plus.number_of_edges(), len_path\n",
    "\n",
    "def generate_vertex(graph, fs, X, len_fs=3):\n",
    "    l = X.shape[0]\n",
    "    feat_target = np.empty((l, len_fs))\n",
    "    feat_source = np.empty((l, len_fs))\n",
    "    t1 = time()\n",
    "    for i, x in enumerate(X):\n",
    "        t=x[0]\n",
    "        s=x[1]\n",
    "        feat_target[i]=fs(graph, t)\n",
    "        feat_source[i]=fs(graph, s)\n",
    "        if i%10000==0:\n",
    "             print(\"{}/{} completed\".format(i,len(X)))\n",
    "    print(\"VERTEX generation time taken\",time()-t)\n",
    "    return feat_target, feat_source\n",
    "\n",
    "def generate_edge(graph, fs, X, len_fs=6):\n",
    "    l = X.shape[0]\n",
    "    feat_edge = np.empty((l, len_fs))\n",
    "    t1 = time()\n",
    "    for i, x in enumerate(X):\n",
    "        t=x[0]\n",
    "        s=x[1]\n",
    "        feat_edge[i]=fs(graph, t, s)\n",
    "        if i%10000==0:\n",
    "            print(\"{}/{} completed\".format(i,len(X)))\n",
    "    print(\"EDGE_genetration_time_taken :\",time()-t1)\n",
    "    return feat_edge\n",
    "\n",
    "def generate_algo(graph, X):\n",
    "    res_alloc_index=np.asarray(list(nx.resource_allocation_index(graph, X)))[:,2]\n",
    "    jac_coef=np.asarray(list(nx.jaccard_coefficient(graph, X)))[:,2]\n",
    "    ad_adar_idx = np.asarray(list(nx.adamic_adar_index(graph, X)))[:,2]\n",
    "    pref_att = np.asarray(list(nx.preferential_attachment(graph, X)))[:,2]\n",
    "    return list(res_alloc_index), list(jac_coef), list(ad_adar_idx), list(pref_att)\n",
    "\n",
    "def generate_numbers(graph, X):\n",
    "    num_target = np.empty((X.shape[0], 3))\n",
    "    num_source = np.empty((X.shape[0], 3))\n",
    "    core_num = nx.core_number(graph)\n",
    "    clus = nx.clustering(graph)\n",
    "    page_rank = nx.pagerank(graph)\n",
    "    t1 = time()\n",
    "    for i, x in enumerate(X):\n",
    "        num_target[i, 0]=core_num[x[0]]\n",
    "        num_target[i, 1]=clus[x[0]]\n",
    "        num_target[i, 2]=page_rank[x[0]]\n",
    "        num_source[i, 0]=core_num[x[1]]\n",
    "        num_source[i, 1]=clus[x[1]]\n",
    "        num_source[i, 2]=page_rank[x[1]]\n",
    "        if i%10000==0:\n",
    "            print(\"{}/{} completed\".format(i,len(X)))\n",
    "    print(\"number_genetration_time_taken :\",time()-t1)\n",
    "    return num_target, num_source\n",
    "\n",
    "def all_oriented_vertex(graph, v):\n",
    "    neighbors_in = graph.predecessors(v)\n",
    "    neighbors_out = graph.successors(v)\n",
    "    neighbors = list(set(neighbors_in).union(neighbors_out))\n",
    "    neighbors_plus = neighbors + [v]\n",
    "    subgraph = graph.subgraph(neighbors)\n",
    "    subgraph_plus = graph.subgraph(neighbors_plus)\n",
    "    scc = nx.number_strongly_connected_components(subgraph)\n",
    "    wcc = nx.number_weakly_connected_components(subgraph)\n",
    "    scc_plus = nx.number_strongly_connected_components(subgraph_plus)\n",
    "    return graph.in_degree(v), graph.out_degree(v), scc, wcc, scc_plus, neighbors_in, neighbors_out, neighbors, neighbors_plus\n",
    "\n",
    "def generate_oriented(graph, X):\n",
    "    %%time\n",
    "    target_feats=np.empty((X.shape[0], 5))\n",
    "    source_feats=np.empty((X.shape[0], 5))\n",
    "    edge_feats = np.empty((X.shape[0], 11))\n",
    "    l = X.shape[0]\n",
    "    for i, x in enumerate(X):\n",
    "        t=x[0]\n",
    "        s=x[1]\n",
    "        in_d_t, out_d_t, scc_t, wcc_t, sccp_t, n_in_t, n_out_t, n_t, np_t = all_oriented_vertex(graph, t)\n",
    "        in_d_s, out_d_s, scc_s, wcc_s, sccp_s, n_in_s, n_out_s, n_s, np_s = all_oriented_vertex(graph, s)\n",
    "        com_in = len(set(n_in_t).intersection(n_in_s))\n",
    "        com_on = len(set(n_out_t).intersection(n_out_s))\n",
    "        trans_ts = len(set(n_out_t).intersection(n_in_s))\n",
    "        trans_st = len(set(n_out_s).intersection(n_in_t))\n",
    "        friends_measure_st=0\n",
    "        friends_measure_ts=0\n",
    "        for ns in n_s:\n",
    "            for nt in n_t:\n",
    "                if graph.has_edge(ns, nt):\n",
    "                    friends_measure_st+=1\n",
    "                if graph.has_edge(nt, ns):\n",
    "                    friends_measure_ts+=1\n",
    "        nh = list(set(n_t).union(n_s))            \n",
    "        nh_plus = list(set(np_t).union(np_s))\n",
    "        sub_nh = graph.subgraph(nh)\n",
    "        sub_nh_plus = graph.subgraph(nh_plus)\n",
    "        scc = nx.number_strongly_connected_components(sub_nh)\n",
    "        wcc = nx.number_weakly_connected_components(sub_nh)\n",
    "        scc_plus = nx.number_strongly_connected_components(sub_nh_plus)\n",
    "        if not nx.has_path(graph, s, t):\n",
    "            len_path_st=-1\n",
    "        else:\n",
    "            len_path_st = nx.shortest_path_length(graph, s, t)\n",
    "        if not nx.has_path(graph, t, s):\n",
    "            len_path_ts=-1\n",
    "        else:\n",
    "            len_path_ts = nx.shortest_path_length(graph, t, s)\n",
    "        target_feats[i]=[in_d_t, out_d_t, scc_t, wcc_t, sccp_t]\n",
    "        source_feats[i]=[in_d_s, out_d_s, scc_s, wcc_s, sccp_s]\n",
    "        edge_feats[i]=[com_in, com_on, trans_ts, trans_st, friends_measure_st, friends_measure_ts, scc, wcc, scc_plus, len_path_st, len_path_ts]\n",
    "        if i%10000==0:\n",
    "            print(i, l)\n",
    "            t2=time()\n",
    "            print(t2-t1)\n",
    "            t1=t2\n",
    "    return target_feats, source_feats, edge_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "621223f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_features(train, K=5):\n",
    "    X = train[['Target', 'Source']].values\n",
    "    y = train[['Edge']].values\n",
    "    target_feats=np.empty((train.shape[0], 5))\n",
    "    source_feats=np.empty((train.shape[0], 5))\n",
    "    edge_feats=np.empty((train.shape[0], 11))\n",
    "    np.random.seed(7)\n",
    "    cv = KFold(n_splits = K, shuffle = True, random_state=1)\n",
    "    for i, (idx_train, idx_val) in enumerate(cv.split(train)):\n",
    "        print(\"CV ITERATION {}\".format())\n",
    "        X_train = X[idx_train]\n",
    "        y_train = y[idx_train]\n",
    "        X_valid = X[idx_val]\n",
    "        y_valid = X[idx_val]\n",
    "        print(\"Creating graph\")\n",
    "        graph = create_directed_graph(X_train, y_train)\n",
    "        print(\"Generating vertex features\")\n",
    "        feat_target, feat_source, feat_edge = generate_oriented(graph, X_valid)\n",
    "        target_feats[idx_val] = feat_target\n",
    "        source_feats[idx_val] = feat_source\n",
    "        edge_feats[idx_val]=feat_edge\n",
    "   \n",
    "    return target_feats, source_feats, edge_feats\n",
    "\n",
    "def generate_graph_features_test(train, test):\n",
    "    t=time()\n",
    "    X = train[['Target', 'Source']].values\n",
    "    y = train[['Edge']].values\n",
    "    X_test = test[['Target', 'Source']].values\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    print(\"Creating graph\")\n",
    "    graph = create_directed_graph(X_train, y_train)\n",
    "    print(\"Generating vertex features\")\n",
    "    feat_target, feat_source, feat_edge = generate_oriented(graph, X_test)\n",
    "    return feat_target, feat_source, feat_edge\n",
    "\n",
    "def give_graph_features(df,feat_target, feat_source, feat_edge):\n",
    "    df['Target_indegree'] = feat_target[:,0]\n",
    "    df['Target_outdegree'] = feat_target[:,1]\n",
    "    df['Target_scc'] = feat_target[:,2]\n",
    "    df['Target_wcc'] = feat_target[:,3]\n",
    "    df['Target_scc_plus'] = feat_target[:,4]\n",
    "    \n",
    "    df['Source_indegree'] = feat_source[:,0]\n",
    "    df['Source_outdegree'] = feat_source[:,1]\n",
    "    df['Source_scc'] = feat_source[:,2]\n",
    "    df['Source_wcc'] = feat_source[:,3]\n",
    "    df['Source_scc_plus'] = feat_source[:,4]\n",
    "    \n",
    "    df['Common_in'] = feat_edge[:,0]\n",
    "    df['Common_out'] = feat_edge[:,1]\n",
    "    df['Transitive_ts'] = feat_edge[:,2]\n",
    "    df['Transitive_st'] = feat_edge[:,3]\n",
    "    df['Friend_measure_st'] = feat_edge[:,4]\n",
    "    df['Friend_measure_ts'] = feat_edge[:,5]\n",
    "    df['Scc'] = feat_edge[:,6]\n",
    "    df['Wcc'] = feat_edge[:,7]\n",
    "    df['Scc_plus'] = feat_edge[:,8]\n",
    "    df['Len_path_st'] = feat_edge[:,9]\n",
    "    df['Len_path_ts'] = feat_edge[:,10]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f555df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph\n",
      "Generating vertex features\n",
      "0 32648\n",
      "0.08083271980285645\n",
      "10000 32648\n",
      "411.36806178092957\n",
      "20000 32648\n",
      "421.66673278808594\n",
      "30000 32648\n",
      "404.797518491745\n"
     ]
    }
   ],
   "source": [
    "tf,sf,ef = generate_graph_features_test(train_semantics,test_semantics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e6289c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_semantics = give_graph_features(test_semantics,tf,sf,ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "444f4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_semantics.to_csv('test_sem_graph1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "50fb44b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32648, 45)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_semantics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ffb1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_semantics = pd.read_csv('train_sem_graph1.csv')\n",
    "# train_semantics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af8d21",
   "metadata": {},
   "source": [
    "##### Features related to vertex and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "62ed1aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_vertex(train,K=3):\n",
    "    X = train[['Target', 'Source']].values\n",
    "    y = train[['Edge']].values\n",
    "    target_vertex=np.empty((train.shape[0], 3))\n",
    "    source_vertex=np.empty((train.shape[0], 3))\n",
    "    np.random.seed(7)\n",
    "    cv = KFold(n_splits = K, shuffle = True, random_state=1)\n",
    "    for i, (idx_train, idx_val) in enumerate(cv.split(train)):\n",
    "        X_train = X[idx_train]\n",
    "        y_train = y[idx_train]\n",
    "        X_valid = X[idx_val]\n",
    "        y_valid = X[idx_val]\n",
    "        print(\"Creating graph\")\n",
    "        graph =create_graph(X_train, y_train)\n",
    "        print(\"Generating vertex features\")\n",
    "        vertex_target, vertex_source = generate_vertex(graph, all_vertex, X_valid)\n",
    "        target_vertex[idx_val] = vertex_target\n",
    "        source_vertex[idx_val] = vertex_source\n",
    "    return target_vertex,source_vertex \n",
    "\n",
    "def give_number(train,K=3):\n",
    "    X = train[['Target', 'Source']].values\n",
    "    y = train[['Edge']].values\n",
    "    target_num=np.empty((train.shape[0], 3))\n",
    "    source_num=np.empty((train.shape[0], 3))\n",
    "    np.random.seed(7)\n",
    "    cv = KFold(n_splits = K, shuffle = True, random_state=1)\n",
    "    for i, (idx_train, idx_val) in enumerate(cv.split(train)):\n",
    "        X_train = X[idx_train]\n",
    "        y_train = y[idx_train]\n",
    "        X_valid = X[idx_val]\n",
    "        y_valid = X[idx_val]\n",
    "        print(\"Creating graph\")\n",
    "        graph =create_graph(X_train, y_train)\n",
    "        print(\"Generating Number features\")\n",
    "        num_target, num_source = generate_numbers(graph, X_valid)\n",
    "        target_num[idx_val] = num_target\n",
    "        source_num[idx_val] = num_source\n",
    "    return target_num,source_num \n",
    "\n",
    "def give_edge(train,K=3):\n",
    "    X = train[['Target', 'Source']].values\n",
    "    y = train[['Edge']].values\n",
    "    feat_edge=np.empty((train.shape[0], 6))\n",
    "    np.random.seed(7)\n",
    "    cv = KFold(n_splits = K, shuffle = True, random_state=1)\n",
    "    for i, (idx_train, idx_val) in enumerate(cv.split(train)):\n",
    "        X_train = X[idx_train]\n",
    "        y_train = y[idx_train]\n",
    "        X_valid = X[idx_val]\n",
    "        y_valid = X[idx_val]\n",
    "        print(\"Creating graph\")\n",
    "        graph =create_graph(X_train, y_train)\n",
    "        print(\"Generating Edge features\")\n",
    "        edge_feat = generate_edge(graph,all_edges, X_valid)\n",
    "        feat_edge[idx_val]=edge_feat\n",
    "        \n",
    "    return feat_edge \n",
    "\n",
    "def generate_train_features(train, K=3, fs=all_edges, len_fs=6):\n",
    "    print(\"Generating vertex features\")\n",
    "    vertex_target, vertex_source = give_vertex(train,K)\n",
    "    \n",
    "    print(\"Generate numbers\")\n",
    "    number_target, number_source = give_number(train,K)\n",
    "    print(\"Generate edges\")\n",
    "    feat_edge = give_edge(train,K)\n",
    "    return (vertex_target,vertex_source,number_target,number_source,feat_edge)\n",
    "\n",
    "def generate_test_features(train, test,fs=all_edges, len_fs=6):\n",
    "\n",
    "    X = train[['Target', 'Source']].values\n",
    "    X_test = test[['Target', 'Source']].values\n",
    "    y = train[['Edge']].values\n",
    "\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    print(\"Creating graph\")\n",
    "    graph = create_graph(X_train, y_train)\n",
    "    print(\"Generating vertex features\")\n",
    "    vertex_target, vertex_source = generate_vertex(graph, all_vertex, X_test)\n",
    "    print(\"Generate numbers\")\n",
    "    number_target, number_source = generate_numbers(graph, X_test)\n",
    "    print(\"Generate edges\")\n",
    "    feat_edge = generate_edge(graph, all_edges, X_test)\n",
    "    return (vertex_target,vertex_source,number_target,number_source,feat_edge)\n",
    "\n",
    "def give_another_graph_features(df,vertex_target,vertex_source,number_target,number_source,feat_edge):\n",
    "    df['Target_degree'] = vertex_target[:,0] \n",
    "    df['Target_nh_subgraph_edges'] = vertex_target[:,1]\n",
    "    df['Target_nh_subgraph_edges_plus'] = vertex_target[:,2] \n",
    "    df['Source_degree'] = vertex_source[:,0] \n",
    "    df['Source_nh_subgraph_edges'] = vertex_source[:,1] \n",
    "    df['Source_nh_subgraph_edges_plus'] = vertex_source[:,2] \n",
    "    \n",
    "    df['Target_core'] = number_target[:,0] \n",
    "    df['Target_clustering'] = number_target[:,1]\n",
    "    df['Target_pagerank'] = number_target[:,2] \n",
    "    df['Source_core'] = number_source[:,0] \n",
    "    df['Source_clustering'] = number_source[:,1]\n",
    "    df['Source_pagerank'] = number_source[:,2] \n",
    "    \n",
    "    df['Common_friends'] = feat_edge[:,0]\n",
    "    df['Total_friends'] = feat_edge[:,1]\n",
    "    df['Friends_measure'] = feat_edge[:,2]\n",
    "    df['Sub_nh_edges'] = feat_edge[:,3]\n",
    "    df['Sub_nh_edges_plus'] = feat_edge[:,4]\n",
    "    df['Len_path'] = feat_edge[:,5]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b3c1b630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating vertex features\n",
      "Creating graph\n",
      "Generating vertex features\n",
      "0/205171 completed\n",
      "10000/205171 completed\n",
      "20000/205171 completed\n",
      "30000/205171 completed\n",
      "40000/205171 completed\n",
      "50000/205171 completed\n",
      "60000/205171 completed\n",
      "70000/205171 completed\n",
      "80000/205171 completed\n",
      "90000/205171 completed\n",
      "100000/205171 completed\n",
      "110000/205171 completed\n",
      "120000/205171 completed\n",
      "130000/205171 completed\n",
      "140000/205171 completed\n",
      "150000/205171 completed\n",
      "160000/205171 completed\n",
      "170000/205171 completed\n",
      "180000/205171 completed\n",
      "190000/205171 completed\n",
      "200000/205171 completed\n",
      "VERTEX generation time taken 1647515010.7882771\n",
      "Creating graph\n",
      "Generating vertex features\n",
      "0/205171 completed\n",
      "10000/205171 completed\n",
      "20000/205171 completed\n",
      "30000/205171 completed\n",
      "40000/205171 completed\n",
      "50000/205171 completed\n",
      "60000/205171 completed\n",
      "70000/205171 completed\n",
      "80000/205171 completed\n",
      "90000/205171 completed\n",
      "100000/205171 completed\n",
      "110000/205171 completed\n",
      "120000/205171 completed\n",
      "130000/205171 completed\n",
      "140000/205171 completed\n",
      "150000/205171 completed\n",
      "160000/205171 completed\n",
      "170000/205171 completed\n",
      "180000/205171 completed\n",
      "190000/205171 completed\n",
      "200000/205171 completed\n",
      "VERTEX generation time taken 1637820038.6169357\n",
      "Creating graph\n",
      "Generating vertex features\n",
      "0/205170 completed\n",
      "10000/205170 completed\n",
      "20000/205170 completed\n",
      "30000/205170 completed\n",
      "40000/205170 completed\n",
      "50000/205170 completed\n",
      "60000/205170 completed\n",
      "70000/205170 completed\n",
      "80000/205170 completed\n",
      "90000/205170 completed\n",
      "100000/205170 completed\n",
      "110000/205170 completed\n",
      "120000/205170 completed\n",
      "130000/205170 completed\n",
      "140000/205170 completed\n",
      "150000/205170 completed\n",
      "160000/205170 completed\n",
      "170000/205170 completed\n",
      "180000/205170 completed\n",
      "190000/205170 completed\n",
      "200000/205170 completed\n",
      "VERTEX generation time taken 1647717000.7499766\n",
      "Generate numbers\n",
      "Creating graph\n",
      "Generating Number features\n",
      "0/205171 completed\n",
      "10000/205171 completed\n",
      "20000/205171 completed\n",
      "30000/205171 completed\n",
      "40000/205171 completed\n",
      "50000/205171 completed\n",
      "60000/205171 completed\n",
      "70000/205171 completed\n",
      "80000/205171 completed\n",
      "90000/205171 completed\n",
      "100000/205171 completed\n",
      "110000/205171 completed\n",
      "120000/205171 completed\n",
      "130000/205171 completed\n",
      "140000/205171 completed\n",
      "150000/205171 completed\n",
      "160000/205171 completed\n",
      "170000/205171 completed\n",
      "180000/205171 completed\n",
      "190000/205171 completed\n",
      "200000/205171 completed\n",
      "number_genetration_time_taken : 0.3721470832824707\n",
      "Creating graph\n",
      "Generating Number features\n",
      "0/205171 completed\n",
      "10000/205171 completed\n",
      "20000/205171 completed\n",
      "30000/205171 completed\n",
      "40000/205171 completed\n",
      "50000/205171 completed\n",
      "60000/205171 completed\n",
      "70000/205171 completed\n",
      "80000/205171 completed\n",
      "90000/205171 completed\n",
      "100000/205171 completed\n",
      "110000/205171 completed\n",
      "120000/205171 completed\n",
      "130000/205171 completed\n",
      "140000/205171 completed\n",
      "150000/205171 completed\n",
      "160000/205171 completed\n",
      "170000/205171 completed\n",
      "180000/205171 completed\n",
      "190000/205171 completed\n",
      "200000/205171 completed\n",
      "number_genetration_time_taken : 0.3248424530029297\n",
      "Creating graph\n",
      "Generating Number features\n",
      "0/205170 completed\n",
      "10000/205170 completed\n",
      "20000/205170 completed\n",
      "30000/205170 completed\n",
      "40000/205170 completed\n",
      "50000/205170 completed\n",
      "60000/205170 completed\n",
      "70000/205170 completed\n",
      "80000/205170 completed\n",
      "90000/205170 completed\n",
      "100000/205170 completed\n",
      "110000/205170 completed\n",
      "120000/205170 completed\n",
      "130000/205170 completed\n",
      "140000/205170 completed\n",
      "150000/205170 completed\n",
      "160000/205170 completed\n",
      "170000/205170 completed\n",
      "180000/205170 completed\n",
      "190000/205170 completed\n",
      "200000/205170 completed\n",
      "number_genetration_time_taken : 0.3415060043334961\n",
      "Generate edges\n",
      "Creating graph\n",
      "Generating Edge features\n",
      "0/205171 completed\n",
      "10000/205171 completed\n",
      "20000/205171 completed\n",
      "30000/205171 completed\n",
      "40000/205171 completed\n",
      "50000/205171 completed\n",
      "60000/205171 completed\n",
      "70000/205171 completed\n",
      "80000/205171 completed\n",
      "90000/205171 completed\n",
      "100000/205171 completed\n",
      "110000/205171 completed\n",
      "120000/205171 completed\n",
      "130000/205171 completed\n",
      "140000/205171 completed\n",
      "150000/205171 completed\n",
      "160000/205171 completed\n",
      "170000/205171 completed\n",
      "180000/205171 completed\n",
      "190000/205171 completed\n",
      "200000/205171 completed\n",
      "EDGE_genetration_time_taken : 1107.7985708713531\n",
      "Creating graph\n",
      "Generating Edge features\n",
      "0/205171 completed\n",
      "10000/205171 completed\n",
      "20000/205171 completed\n",
      "30000/205171 completed\n",
      "40000/205171 completed\n",
      "50000/205171 completed\n",
      "60000/205171 completed\n",
      "70000/205171 completed\n",
      "80000/205171 completed\n",
      "90000/205171 completed\n",
      "100000/205171 completed\n",
      "110000/205171 completed\n",
      "120000/205171 completed\n",
      "130000/205171 completed\n",
      "140000/205171 completed\n",
      "150000/205171 completed\n",
      "160000/205171 completed\n",
      "170000/205171 completed\n",
      "180000/205171 completed\n",
      "190000/205171 completed\n",
      "200000/205171 completed\n",
      "EDGE_genetration_time_taken : 1102.5118207931519\n",
      "Creating graph\n",
      "Generating Edge features\n",
      "0/205170 completed\n",
      "10000/205170 completed\n",
      "20000/205170 completed\n",
      "30000/205170 completed\n",
      "40000/205170 completed\n",
      "50000/205170 completed\n",
      "60000/205170 completed\n",
      "70000/205170 completed\n",
      "80000/205170 completed\n",
      "90000/205170 completed\n",
      "100000/205170 completed\n",
      "110000/205170 completed\n",
      "120000/205170 completed\n",
      "130000/205170 completed\n",
      "140000/205170 completed\n",
      "150000/205170 completed\n",
      "160000/205170 completed\n",
      "170000/205170 completed\n",
      "180000/205170 completed\n",
      "190000/205170 completed\n",
      "200000/205170 completed\n",
      "EDGE_genetration_time_taken : 1110.5709626674652\n"
     ]
    }
   ],
   "source": [
    "train_ftrs = generate_train_features(train_semantics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c0af9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e = train_ftrs[0],train_ftrs[1],train_ftrs[2],train_ftrs[3],train_ftrs[4]\n",
    "train_semantics = give_another_graph_features(train_semantics,a,b,c,d,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5ea12761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615512, 63)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_semantics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52af620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_semantics.to_csv('train_total.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b74ddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph\n",
      "Generating vertex features\n",
      "0/32648 completed\n",
      "10000/32648 completed\n",
      "20000/32648 completed\n",
      "30000/32648 completed\n",
      "VERTEX generation time taken 1647523506.5052304\n",
      "Generate numbers\n",
      "0/32648 completed\n",
      "10000/32648 completed\n",
      "20000/32648 completed\n",
      "30000/32648 completed\n",
      "number_genetration_time_taken : 0.056294918060302734\n",
      "Generate edges\n",
      "0/32648 completed\n",
      "10000/32648 completed\n",
      "20000/32648 completed\n",
      "30000/32648 completed\n",
      "EDGE_genetration_time_taken : 379.2655680179596\n"
     ]
    }
   ],
   "source": [
    "test_ftrs = generate_test_features(train_semantics,test_semantics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2babb0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32648, 63)\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d,e = test_ftrs[0],test_ftrs[1],test_ftrs[2],test_ftrs[3],test_ftrs[4]\n",
    "test_semantics = give_another_graph_features(test_semantics,a,b,c,d,e)\n",
    "print(test_semantics.shape)\n",
    "test_semantics.to_csv('test_total.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
